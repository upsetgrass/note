[TOC]

# 文件系统

https://segmentfault.com/a/1190000023615225

## 基本概念

文件系统是操作系统中负责管理长期数据的子系统，负责管理计算机的存储设备上的长期数据，主要功能是组织、管理数据的存储、访问和保护，提供了对磁盘空间的抽象，使得操作系统能够以结构化的方式来存储和读取文件。

文件系统的基本数据单位是文件，按照不同的组织方式，会形成不同的文件系统

Linux 最经典的一句话是：「**一切皆文件**」，不仅普通的文件和目录，就连块设备、管道、socket 等，也都是统一交给文件系统管理的

ps：块设备：硬盘，SSD等提供数据存储的介质；管道：进程间通信（IPC）机制，允许一个进程的输出作为另一个进程的输入；socket：网络通信的编程接口，提供不同计算机之间的数据传输能力。

Linux文件系统会为每个文件分配两个数据结构：**索引节点**（index node）和**目录项**（directory entry），主要用来记录文件的元信息和目录层次结构

### 索引节点

索引节点：也就是 inode，用来记录文件的元信息（除了文件名之外的所有重要属性），比如 inode 编号、文件大小、访问权限、创建时间、修改时间、硬链接计数（多少个目录项指向该inode）、数据块指针（文件实际数据存放的磁盘位置，可以是直接指针，也可以是间接指针）等等。

inode节点是文件的**唯一**标识，也同样都会被存储在硬盘中，所以索引节点同样占用磁盘空间，并且索引节点都被存储在磁盘的指定位置并按照顺序存储在inode表，而inode表空间是有限的，也就是文件数量有上限（几乎不会遇到因文件数量超出inode表空间的情况）。而为了加速文件访问，会把inode节点加载到内存cache中。

inode是文件的物理表示，文件名是指向一个inode的逻辑表示，因而不是本质属性，并且将文件名和inode解构，使得文件系统能够支持硬链接，使得多个文件名指向同一个inode，也可以通过特殊的 inode（软链接）引用其他路径，从而实现灵活的链接机制

### 目录项

目录项：也就是dentry，用来记录文件的名字、**inode编号**以及 与其他目录项的层级关联关系（父子关系/映射关系）。多个目录项关联起来，目录项用于实现文件系统的层级结构。例如，根目录、子目录、文件等，都通过目录项建立起从根目录到文件的路径关系。目录项的存在使得文件系统能够通过文件路径（文件名）找到对应的 inode编号，从而找到inode，获取文件的元数据和数据内容。简而言之就是，文件名→inode

目录项不是直接存储在磁盘上的。它们由内核在内存中管理，缓存在内存中以提高访问效率。当你列出目录中的文件（ls）时，实际上是内核通过查询内存中的目录项来获取文件名和它们对应的 inode。

文件系统通过目录项建立文件路径，例如 `/home/user/file.txt`，每一层目录（`home`、`user`、`file.txt`）都有一个对应的目录项，指向该层的 inode。当访问文件 `/home/user/file.txt` 时，内核会通过目录项找到 `home` 目录的 inode，再通过 `home` 的目录项找到 `user` 目录的 inode，依此类推，最终访问 `file.txt` 的 inode，获得文件的元数据。

文件系统会把存储空间划分成多个块组（block group），每一个block group中有一个inode表，inode表是一个数组，该数组存储了多个inode结构体，还有一个数据块区域，存储了实际数据或者一组目录项→inode编号，inode结构体中的数据块指针指向了对应的数据块。通过inode编号可以寻找到inode所在的块组，以及在inode表中第几个inode（偏移）

![image-20250407144336830](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250407144336830.png)



inode编号是文件在磁盘上的唯一标识

## inode信息查看

想要查看一个文件的inode编号，可以通过指令`ls -i [文件名]`

对普通文件ls -i时查看的就是文件本身的inode编号，而对于目录文件来说，ls -i 查看的就是目录中的文件的inode编号

![image-20250402100120223](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250402100120223.png)

想要查看一个inode的详细信息，可以`stat [文件名]` ，比如`stat /`  `stat file.txt`

![image-20250402095554728](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250402095554728.png)

![image-20250402095628915](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250402095628915.png)

![image-20250402102007978](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250402102007978.png)



## 软硬链接

在linux文件系统中，目录的硬链接计数是代表了指向该目录文件的目录项数量，这里~/git_box/note/test目录的硬链接是5，是因为当前目录下有三个子目录，子目录中存储的..就是当前目录，所以子目录有三个硬链接，对于当前目录也有一个 . 指向当前目录，对于上一级目录~/git_box/note，可以通过test指向当前目录，所以总共5个硬链接

![image-20250402102530978](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250402102530978.png)

对于普通文件，硬链接数量是指有多少个目录项（文件名）指向同一个inode，可以通过`ln [src] [dest]`创建新的文件名指向同一个inode

![image-20250402105602066](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250402105602066.png)

如果想要创建软连接，那么就是用`ln -s [src] [dest]`

软链接 vs 硬链接：软连接存储的只是一个路径，一个指向，指向了源文件，源文件移动/删除，软连接文件无法使用；硬链接与源文件共享同一个inode编号，删除一个不会影响磁盘中的存储内容，直到该inode编号的硬链接数量变为了0，才会删除存储内容。

![image-20250402111221700](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250402111221700.png)

## 访问流程

用户访问某个文件以cat /home/file1.txt命令为例：

首先通过超级块superblock获取根目录/的inode编号寻找到inode，然后在inode的数据块中寻找到home→1001，也就是找到home目录项，然后在home目录项的inode编号寻找到对应的inode，进而找到file1.txt→1005，然后访问到file1.txt目录块，此时再次寻找到file1.txt的inode的数据块，就能够访问到file1.txt的实际数据。

![image-20250402162349357](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250402162349357.png)

## 数据块

磁盘读写的最小单位是扇区，512Byte，文件系统把多个扇区组成了一个逻辑块，每次读写的最小单位就是逻辑块，也就是数据块，Linux中的逻辑块是4KB，一次性读取8个扇区。

另外，磁盘进行格式化的时候，会被分成三个存储区域，分别是超级块、索引节点区和数据块区。

- 超级块，用来存储文件系统的详细信息，比如块个数、块大小、空闲块等等，每个文件系统有一个超级块。
- 索引节点区（inode表），用来存储索引节点；
- 数据块区，用来存储文件或目录数据；

我们不可能把超级块和索引节点区全部加载到内存，这样内存肯定撑不住，所以只有当需要使用的时候，才将其加载进内存，它们加载进内存的时机是不同的：

- 超级块：当文件系统挂载时加载进内存，用于管理文件系统的运行状态；
- 索引节点区：当文件被访问时，系统根据inode号加载对应的 inode 结构到内存；





## 挂载

挂载（Mounting） 是指 将一个文件系统（例如 ext4、xfs、ntfs 等）与 Linux 目录树中的某个目录关联并加载到内存中（关联的目录可以不为空，但挂载后这个目录下以前的内容将不可用，直到umount卸载后内容才能够重新可见），使得CPU能够通过该目录访问该文件系统中的数据。

磁盘上的文件系统在没有挂载之前，数据是不可访问的。挂载后，文件系统就能通过 Linux 目录树访问，用户可以像访问普通目录一样访问该磁盘的内容。

（这里说的文件系统可以理解为一个目录树，比如 / 是一个文件系统， /proc 也是一个文件系统）

Linux维护了一个挂载表（/proc/mounts）记录当前所有挂载的文件系统，`cat /proc/mounts`或  `findmnt`即可查看挂载的内容

/etc/fstab用于定义系统启动时应该挂载的**持久性/静态性文件系统**（如根分区 `/`、EFI- `/boot/efi` 等），它不会列出运行时动态挂载的文件系统，比如 `tmpfs`、`sysfs`、`procfs`、`cgroup` 等。除了/etc/fstab，还有其他的例如：/run/mount  /etc/systemd/system/\*.mount  /etc/udev/rules.d/等会自动挂载

（/etc/mtab是/proc/mounts的符号链接，因此内容和后者完全一致）

![image-20250403114121709](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250403114121709.png)

/proc/mounts里面有两个，一个是根文件系统 / ，一个是EFI分区 /boot/efi  （EFI/UEFI用于启动操作系统、初始化硬盘、提供驱动程序与服务、强大的配置功能等）

![image-20250402180059346](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250402180059346.png)

![image-202504021737233161](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250402175608043.png)

根目录/是一个ext4文件系统类型，存储在UUID=7b...设备上，/boot/efi是一个vfat文件系统类型，存储在UUID=54...设备上

![image-20250407152551495](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250407152551495.png)

/proc/mounts是动态生成的文件，是当前已经挂载的所有文件系统，包括了：

1、虚拟文件系统（内核自动挂载）

```shell
sysfs /sys
proc /proc
tmpfs /run
cgroup2 /sys/fs/cgroup
debugfs /sys/kernel/debug
tracefs /sys/kernel/tracing
```

2、设备管理相关

```
udev /dev
devpts /dev/pts
Snap 相关的 SquashFS
/dev/loopX（比如 /snap/firefox/5889）
```

3、用户进程动态挂载

```
/run/user/1000
fuse.gvfsd-fuse
portal /run/user/1000/doc
```



![image-20250402175558954](../pic_to_typora/image-20250402175558954.png)





存储设备都需要挂载到一个文件系统，才能被操作系统管理和使用：

存储设备（硬盘、U 盘等）本质上只是“原始块设备”，不能直接存储文件。必须：

先在设备上 创建文件系统（如 `mkfs.ext4` ``/dev/sdb1`）

再挂载文件系统到某个目录（如` mount /dev/sdb1 /mnt/usb`）

之后，系统才能在 /mnt/usb 这个目录下读写文件



查看系统有哪些块设备：`lsblk`

最后一列就是挂载情况，没有任何显示的就是没有挂载

![image-20250403110023326](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250403110023326.png)

sdb1我这边没有，是因为我的是nvme硬盘，而不是sdb硬盘

也可以用`lsblk -f`可以显示更加详细的信息：文件系统类型FSTYPE，UUID、磁盘可控空间FSAVAIL



### 挂载对象

1、块设备（存储设备）：

用于传统存储文件系统，必须挂载到目录才能访问其内容，挂载的对象如下：

硬盘分区（如 /dev/sda1）

U 盘、SD 卡（如 /dev/sdb1）

ISO 光盘镜像（如 /dev/sr0）

RAID 设备（如 /dev/md0）

挂载方式例如：`sudo mount /dev/sdb1 /mnt` 把U盘挂载到/mnt目录

![image-20250403150426393](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250403150426393.png)

ext4、xfs、btrfs这些都是文件系统类型，都是块设备（硬件）可用的存储文件的格式

![image-20250403152429371](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250403152429371.png)

2、伪文件系统：

特点：不存储真实数据，而是提供系统信息或特殊接口

![image-20250403150531067](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250403150531067.png)

```shell
sudo mount -t proc proc /proc
sudo mount -t sysfs sysfs /sys
```

3、网络文件系统：

挂载远程服务器上的存储，使得像本地目录一样使用

![image-20250403152014375](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250403152014375.png)

```shell
sudo mount -t nfs 192.168.1.100:/shared /mnt/nfs
sudo mount -t cifs //192.168.1.200/shared /mnt/smb -o username=user,password=pass
```

4、特殊用途文件系统

增强功能或提供额外支持

![image-20250403152129343](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250403152129343.png)

```shell
sshfs user@remote:/data /mnt/sshfs
```





为什么不能直接访问块设备、伪文件系统、网络文件系统所在的原始目录，而需要挂载到某个指定的文件系统下？

不同类型的块设备和文件系统有不同的访问方式和使用场景，直接访问这些目录会异常麻烦，如下：

![image-20250407112037128](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250407112037128.png)

![image-20250407112101469](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250407112101469.png)

![image-20250407112110500](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250407112110500.png)

由此原因，出现了挂载

1、保证块设备、伪文件系统、网络文件系统被正确识别和访问，并且通过特定的挂载点进行操作

2、挂载提供了统一、标准化的访问接口，使得用户可以通过文件系统的方式访问各种设备和资源，而不需要关心它们的内部工作细节。



因为/dev/sdb1是原始块设备，是二进制数据流，通过mount挂载，可以把sdb1上的文件系统解析出来，把它的目录结构映射到 /mnt/usb，这样就可以以“文件和目录”的方式访问U盘，而不是直接操作二进制数据。



## 将完整磁盘.img挂载到本机

使用kpartx自动生成分区映射

`sudo kpartx -av ubuntu-24.10-preinstalled-server-riscv64.img`

![image-20250825102758192](https://raw.githubusercontent.com/upsetgrass/typora_pic_bed/main/image-20250825102758192.png)

此时出现的loop3p1 - loop3p15就是.img的分区

挂载root分区，体积最大的，所以这里就是loop3p1

`sudo mount /dev/mapper/loop3p1 /mnt/qemu`

此时查看，里面就是磁盘中的内容

`ls /mnt/qemu`

此时就可以对.img中的内容进行修改



修改完后，执行下面的指令将分区映射删除掉（目前实现发现/dev/mapper中的loop内容并为删除，暂未解决该问题）

`sudo kpartx -d ubuntu-24.10-preinstalled-server-riscv64.img` 

### 执行一些作用于.img的指令（.img开机执行）

```
此时.img挂载到了/mnt/qemu，然后需要下面的操作：
写一个/mnt/qemu/etc/systemd/system/myscript.service，用来定义该服务启动时间点，执行方式，**执行内容**，是否开机自启

​``` .service
[Unit]
Description=My Auto Script
After=network.target

[Service]
# 运行用户（即使nologin也没关系，systemd能启动）
User=happy_grass
Group=happy_grass
ExecStart=/opt/guest_auto.sh
Restart=always
# 如果不需要交互终端就用 Type=simple
Type=simple

[Install]
WantedBy=multi-user.target
​```
然后再写一个/opt/guest_auto.sh作为**执行内容**
​```.sh
cd ~
mkdir xxx
sudo mount share_dir
# ...你自己的逻辑
​```

此时就需要进入chroot环境：

# chroot环境中/dev  /proc  /sys是必要的，需要手动挂载起来
sudo mount --bind /dev /mnt/qemu/dev
sudo mount --bind /proc /mnt/qemu/proc
sudo mount --bind /sys /mnt/qemu/sys
# 进入chroot环境
sudo chroot /mnt/qemu /bin/bash
此时终端变成：
root@small-grass:/#
# 并且ls / 就是原来的/mnt/qemu
# 此时我们需要：
sudo chmod +x /opt/guest_auto.sh
# 下面就可以将.service添加作为开机自启
systemctl enable myscript.service
# 此时出现：Created symlink '/etc/systemd/system/multi-user.target.wants/myscript.service' → '/etc/systemd/system/myscript.service'.

# 退出chroot环境
exit

# 卸载
sudo umount /mnt/qemu/proc
sudo umount /mnt/qemu/sys
sudo umount /mnt/qemu/dev
sudo umount /mnt/qemu # 执行该条指令时，注意当前不在/mnt/qemu/路径上
# 删除分区映射
sudo kpartx -d ubuntu-24.10-preinstalled-server-riscv64.img
```



# 固件

固件（Firmware）是介于硬件和操作系统之间的低级软件，负责硬件初始化和平台环境配置。它类似于 x86 的 BIOS 或 UEFI，作用包括：

- 初始化CPU、内存和外围设备
- 设置硬件参数
- 加载引导程序或操作系统内核
- 提供硬件抽象接口（如OpenSBI是RISC-V的固件）

**在RISC-V架构上，OpenSBI（Open Source Supervisor Binary Interface）就是主流的固件，负责启动阶段硬件抽象和中断管理**



`sudo apt-get install opensbi`之后`/usr/lib/riscv64-linux-gnu/opensbi/generic/`目录下就有各种OpenSBI，比如fw_jump.elf





**固件加载后，固件需要找到内核镜像并加载**。

- 这通常通过设备树（`.dtb`）文件或固件默认配置实现。
- 如果没有正确指定内核和设备树，或者固件和内核版本不匹配，可能导致启动停滞。

**固件版本或兼容性问题**。

- 你用的 `fw_jump.elf` 可能是旧版本，或者和你所用内核、QEMU版本不兼容。
- 固件有时会卡在硬件初始化步骤。

**固件期望特定硬件或启动参数**。

- 固件启动时会读取特定硬件配置，如果缺失或错误也会卡住。

**你没给QEMU指定设备树文件（`.dtb`）**

- 固件启动后会加载设备树，如果没给或者设备树错误，也会卡。





